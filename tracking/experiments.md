# Experiments Tracking

**Update frequency:** When starting experiment, when getting results

---

## HOW TO USE

- Document hypothesis BEFORE running experiment
- Track metrics in real-time
- Update results as they come in
- Archive completed experiments to bottom when done

---

## Active Experiments

### Experiment #1: Listening Tour vs Direct Campaign Push

**Started:** January 13, 2026  
**Status:** ðŸŸ¡ Running  
**Next check:** After 10-15 DM conversations (target: January 16)

**Hypothesis:**  
If we build relationships first (feedback conversations) before pitching participation, then we'll get higher quality submissions and better response rates than cold campaign launches.

**Testing:**
- **Cold launch (Days 1-2):** Launched campaigns → Posted announcements → Sent DMs with campaign links
- **Listening tour (Days 3-7):** DM for feedback only → Engage in community spaces → Earn trust before inviting

**Metrics:**

| Metric | Cold Launch | Listening Tour | Target |
|--------|-------------|----------------|--------|
| DM response rate | 15% | TBD | >30% |
| Quality of feedback | N/A | TBD | 5+ actionable insights |
| Time to first submission | 48hr+ (none yet) | TBD | <48hr after invite |
| Submission quality | 0 to measure | TBD | Thoughtful commentary |
| Community recognition | None | TBD | Recognized in spaces |

**Early Results:**
- Cold launch: 200 tweet views, 1 like, 0 submissions, ~15% DM response rate
- Listening tour: Just started, TBD

**Decision criteria:**
- If listening tour gets >30% DM response + quality feedback → Continue approach
- If listening tour leads to first 5 submissions → Validate hypothesis
- If no difference after 7 days → Reconsider approach

---

## Planned Experiments

### Timeline Compression Test
**Question:** Does 48hr submission window create too much delay? Would 24hr create more urgency?

**Plan:**
- Run next campaign with 24hr submission window
- Compare: Submission rate, quality, total participants
- Measure: Does urgency help or hurt quality?

**Start:** After completing first campaign

---

### Follower Tier Optimization
**Question:** Are current tiers ($8-$90) right for Nubcat community? Most holders are <5K followers.

**Plan:**
- Analyze first 20 submissions by follower tier
- Look for gaps: Are we overpaying large accounts? Underpaying small?
- Test new tier structure in Campaign 2

**Start:** After 20 submissions

---

### Quote Tweet vs Simple Retweet
**Question:** Is quote tweet requirement too high a barrier? Would simple retweets get more volume?

**Concerns:**
- Quote tweets = safer for Twitter TOS, higher quality
- Simple retweets = easier, more volume, but TOS risk

**Plan:**
- IF quote tweet campaigns get good participation → Keep it
- IF quote tweet creates too much friction → Test hybrid approach

**Start:** After Campaign 1 completes

---

## Completed Experiments (Archive)

*[Experiments will be moved here when complete with final results and learnings]*

---

## Experiment Template (Copy This for New Tests)

### Experiment #[X]: [Name]

**Started:** [Date]  
**Status:** [Planning / Running / Complete]  
**Next check:** [When to review]

**Hypothesis:**  
If we [action], then [expected outcome] because [reasoning].

**Testing:**
- Control: [What's the baseline?]
- Variable: [What are we changing?]
- Duration: [How long to run?]

**Metrics:**

| Metric | Baseline | Test | Target |
|--------|----------|------|--------|
| [Metric 1] | [#] | TBD | [Goal] |
| [Metric 2] | [#] | TBD | [Goal] |

**Decision criteria:**
- If [result A] → [action]
- If [result B] → [action]

---

**Last updated:** January 13, 2026
